# Multi-Provider LLM System - Quick Start

## ğŸ¯ Overview

You now have a complete multi-provider LLM system that supports 7 different AI providers:

- **Ollama** (Local, free)
- **OpenAI** (GPT-4, GPT-3.5)
- **Anthropic** (Claude)
- **Perplexity** (Sonar)
- **Google** (Gemini)
- **Groq** (Fast Llama)
- **OpenRouter** (Unified access)

## ğŸš€ Quick Start

### 1. Install Dependencies

```bash
pip install -r requirements.txt
```

### 2. Start Backend

```bash
uvicorn backend.main:app --reload
```

### 3. Configure via UI

1. Open Settings page
2. Enable desired providers
3. Add API keys
4. Test connections
5. Select active provider/model
6. Save and start chatting!

## ğŸ“ What Was Implemented

### Backend (Python)

```
backend/model_providers/
â”œâ”€â”€ base.py              # Provider protocol/interface
â”œâ”€â”€ ollama.py            # Local Ollama support
â”œâ”€â”€ openai.py            # OpenAI GPT models
â”œâ”€â”€ anthropic.py         # Anthropic Claude
â”œâ”€â”€ additional.py        # Perplexity, Google, Groq, OpenRouter
â””â”€â”€ __init__.py          # Provider registry & manager
```

**Updated Files:**
- `main.py` - New endpoints & settings system
- `interface.py` - Uses ProviderManager
- `requirements.txt` - Added openai, anthropic, google-generativeai

### Frontend (TypeScript/React)

**Updated Files:**
- `contexts/SettingsContext.tsx` - New provider types
- `pages/Settings.tsx` - Complete rewrite with provider UI
- `styles/settings.css` - Provider card styles

### Documentation

- `docs/multi_provider_system.md` - Complete documentation
- `docs/PROVIDER_QUICKSTART.md` - This file

## ğŸ”‘ Getting API Keys

### OpenAI
1. Go to https://platform.openai.com/api-keys
2. Create new secret key
3. Copy and paste into Settings

### Anthropic
1. Go to https://console.anthropic.com/account/keys
2. Create new API key
3. Copy and paste into Settings

### Google (Gemini)
1. Go to https://makersuite.google.com/app/apikey
2. Create API key
3. Copy and paste into Settings

### Others
- **Perplexity**: https://www.perplexity.ai/settings/api
- **Groq**: https://console.groq.com/keys
- **OpenRouter**: https://openrouter.ai/keys

## ğŸ“ New API Endpoints

```
GET  /providers                    # List all providers
GET  /providers/{id}/models        # Get models for provider
POST /providers/{id}/validate      # Test credentials
GET  /providers/{id}/status        # Check availability
GET  /settings                     # Get settings (new format)
POST /settings                     # Save settings (auto-migrates old format)
```

## ğŸ’¡ Usage Examples

### Via Settings UI (Easiest)

1. Settings â†’ Enable "Anthropic"
2. Enter API key
3. Test Connection
4. Set as active provider
5. Choose "Claude 3.5 Sonnet"
6. Save â†’ Done!

### Programmatically

```python
from backend.model_providers import ProviderManager, Message

manager = ProviderManager(
    active_provider_id="anthropic",
    credentials={"anthropic": {"api_key": "sk-ant-..."}}
)

response = manager.chat([
    Message(role="user", content="What is quantum computing?")
])

print(response.content)
```

## ğŸ”’ Security

- âœ… API keys masked in UI
- âœ… Stored locally in `~/.zotero-llm/settings.json`
- âœ… Never logged or exposed
- âœ… Per-provider validation

## ğŸ¨ Features

### Provider Management
- Enable/disable any provider
- Multiple providers enabled simultaneously
- Switch active provider anytime

### Model Selection
- Dynamic model loading per provider
- Use default or choose specific model
- See model descriptions and context lengths

### Connection Testing
- Test before saving
- Clear error messages
- Status indicators

### Settings Migration
- Old format auto-migrates
- No data loss
- Backward compatible

## ğŸ› Troubleshooting

### Ollama not connecting
```bash
# Make sure Ollama is running
ollama serve

# Pull a model if needed
ollama pull llama3.2
```

### Invalid API key
- Check format (OpenAI: `sk-...`, Anthropic: `sk-ant-...`)
- Verify key is active in provider dashboard
- Test connection in Settings

### Model not found
- For Ollama: Pull model first
- For cloud providers: Check model name is current

## ğŸ“š Documentation

- **Full docs**: `docs/multi_provider_system.md`
- **Architecture details**: See `backend/model_providers/base.py`
- **API reference**: See full docs for all endpoints

## âœ¨ Benefits

1. **Flexibility** - Switch providers without code changes
2. **Privacy** - Use local Ollama for sensitive data
3. **Cost** - Mix free and paid options
4. **Quality** - Choose best model for each task
5. **Future-proof** - Easy to add new providers

## ğŸš¢ Status

âœ… **Production Ready**

All features implemented and tested:
- [x] 7 provider integrations
- [x] Settings UI with provider management
- [x] API endpoints for provider operations
- [x] Automatic settings migration
- [x] Secure credential handling
- [x] Documentation complete

## ğŸ¯ Next Steps (Optional)

Potential enhancements:
- Streaming responses
- Cost tracking
- Model comparison
- Response caching
- Rate limit handling

See `docs/multi_provider_system.md` for details.

---

**Questions?** Check the full documentation in `docs/multi_provider_system.md`
